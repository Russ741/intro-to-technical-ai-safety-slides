+ What exactly is a privileged basis? Basis of the input space I guess?
+ How crucial is the softmax for attention?
    * It seems like it's very crucial because otherwise it's just linear transformations? But maybe not?
        - If it's crucial, then it seems like what's happening is more than "just" normalization?
    * Would like to run GPT-2 without softmax
